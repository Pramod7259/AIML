{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2697344a-f6d3-42b2-8e02-c8128fa75485",
   "metadata": {},
   "source": [
    "# -------------Experiment - 13-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f4a668-7f10-4add-ab3d-a90869b6d321",
   "metadata": {},
   "source": [
    "# Write a NLP Program to demostrate following tasks\n",
    "## a. Tokenization removal of stop words, punchuation, POS & NER Tags\n",
    "## b. Bag of Words, TF-IDF Vectorisation & Ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d123ad-a47e-4d41-b155-59cc7b25f410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfee7cd1-6f62-460c-bcc9-04c19e1d38ea",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d8baf29-1190-431a-a07a-70762eafe38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0ce873-67ca-4bf6-af03-8ca7e1cfef83",
   "metadata": {},
   "source": [
    "## Download required NLTK resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d59aa7be-0a4b-4800-841c-c726bbe78b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Pramoda A\n",
      "[nltk_data]     S\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Pramoda A\n",
      "[nltk_data]     S\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Pramoda A S\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to C:\\Users\\Pramoda\n",
      "[nltk_data]     A S\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to C:\\Users\\Pramoda A\n",
      "[nltk_data]     S\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32fa284-c46d-4746-b764-c456f2e7fbb5",
   "metadata": {},
   "source": [
    "## Sample Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13a53e1b-fe97-409d-ba76-700298e22bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Apple is looking at buying U.K. startup for $1 billion. \n",
    "          Artificial Intelligence is the future of technology!\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1be40f-dff3-4f53-b8a1-fcaa3baa7206",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ff56469-249c-4c6a-813d-7df592ea4887",
   "metadata": {},
   "source": [
    "# a) Tokenization, Stop Words & Punctuation Removal\n",
    "## ------------------------------------------------\n",
    "## Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1ed5e6c-8a00-43fc-a6ee-7783797deb6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Tokens ---\n",
      "['Apple', 'is', 'looking', 'at', 'buying', 'U.K.', 'startup', 'for', '$', '1', 'billion', '.', 'Artificial', 'Intelligence', 'is', 'the', 'future', 'of', 'technology', '!']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(text)\n",
    "print(\"--- Tokens ---\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b111690-3592-496d-be1e-c0c73069a1a0",
   "metadata": {},
   "source": [
    "## Remove stop words and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66a79077-82b5-4bce-abd1-ca60ac5689e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "punctuation_regex = re.compile(r'[\\W_]+')  # matches punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88a62f26-0c78-4244-8d9a-8e1de0b42e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Tokens after Stopword & Punctuation Removal ---\n",
      "['Apple', 'looking', 'buying', 'U.K.', 'startup', '1', 'billion', 'Artificial', 'Intelligence', 'future', 'technology']\n"
     ]
    }
   ],
   "source": [
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words and not punctuation_regex.match(word)]\n",
    "print(\"\\n--- Tokens after Stopword & Punctuation Removal ---\")\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38f0f7e-d8b4-4a1c-9d31-562af72a6beb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "296acc16-9972-4d08-bf82-f071bf9c3338",
   "metadata": {},
   "source": [
    "## POS Tagging & NER using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e46222f5-bede-4b27-b01e-5c1308fed072",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')  # Load small English model\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fb82c5-a526-49de-b80c-21a892eda71a",
   "metadata": {},
   "source": [
    "## POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2803e9fd-95ba-4071-853f-b40823c1f634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- POS Tagging ---\n",
      "Apple        --> PROPN\n",
      "is           --> AUX\n",
      "looking      --> VERB\n",
      "at           --> ADP\n",
      "buying       --> VERB\n",
      "U.K.         --> PROPN\n",
      "startup      --> VERB\n",
      "for          --> ADP\n",
      "$            --> SYM\n",
      "1            --> NUM\n",
      "billion      --> NUM\n",
      ".            --> PUNCT\n",
      "\n",
      "            --> SPACE\n",
      "Artificial   --> PROPN\n",
      "Intelligence --> PROPN\n",
      "is           --> AUX\n",
      "the          --> DET\n",
      "future       --> NOUN\n",
      "of           --> ADP\n",
      "technology   --> NOUN\n",
      "!            --> PUNCT\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- POS Tagging ---\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text:<12} --> {token.pos_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffc2216-541b-4c01-a591-e01cdf9e2117",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8aa92c04-bd3e-4ec6-a484-27bda8f0da6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Named Entities ---\n",
      "Apple           --> ORG\n",
      "U.K.            --> GPE\n",
      "$1 billion      --> MONEY\n",
      "Artificial Intelligence --> PERSON\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Named Entities ---\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text:<15} --> {ent.label_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cef69e-dcd1-4f46-9c51-f5215d6fb092",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65cc73fe-a53f-4ccd-be23-3af8bdf44508",
   "metadata": {},
   "source": [
    "# b. Bag of Words, TF-IDF Vectorisation & NgramsÂ¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45b2e59-3fd5-4e85-ae58-7dd4a0022fdc",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2854b949-4757-456b-9651-13ede34c7a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Bag of Words ---\n",
      "['apple' 'artificial' 'at' 'billion' 'buying' 'for' 'future'\n",
      " 'intelligence' 'is' 'looking' 'of' 'startup' 'technology' 'the']\n",
      "[[1 1 1 1 1 1 1 1 2 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "bow = vectorizer.fit_transform([text])\n",
    "print(\"\\n--- Bag of Words ---\")\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(bow.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ea42ae-0fbe-4e3e-850f-0538085642de",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "075ffb51-36ab-4172-8699-07157f5176f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TF-IDF ---\n",
      "['apple' 'artificial' 'at' 'billion' 'buying' 'for' 'future'\n",
      " 'intelligence' 'is' 'looking' 'of' 'startup' 'technology' 'the']\n",
      "[[0.24253563 0.24253563 0.24253563 0.24253563 0.24253563 0.24253563\n",
      "  0.24253563 0.24253563 0.48507125 0.24253563 0.24253563 0.24253563\n",
      "  0.24253563 0.24253563]]\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf = tfidf_vectorizer.fit_transform([text])\n",
    "print(\"\\n--- TF-IDF ---\")\n",
    "print(tfidf_vectorizer.get_feature_names_out())\n",
    "print(tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62835e5e-bbe8-4c63-b7ff-8e7511b882f5",
   "metadata": {},
   "source": [
    "## N-grams (Bigrams and Trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6728b43e-76db-412d-978f-b02c5cf4bf1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- N-grams (Bigrams and Trigrams) ---\n",
      "['apple is' 'apple is looking' 'artificial intelligence'\n",
      " 'artificial intelligence is' 'at buying' 'at buying startup'\n",
      " 'billion artificial' 'billion artificial intelligence' 'buying startup'\n",
      " 'buying startup for' 'for billion' 'for billion artificial' 'future of'\n",
      " 'future of technology' 'intelligence is' 'intelligence is the'\n",
      " 'is looking' 'is looking at' 'is the' 'is the future' 'looking at'\n",
      " 'looking at buying' 'of technology' 'startup for' 'startup for billion'\n",
      " 'the future' 'the future of']\n",
      "[[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "ngram_vectorizer = CountVectorizer(ngram_range=(2, 3))\n",
    "ngrams = ngram_vectorizer.fit_transform([text])\n",
    "print(\"\\n--- N-grams (Bigrams and Trigrams) ---\")\n",
    "print(ngram_vectorizer.get_feature_names_out())\n",
    "print(ngrams.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7bcbed-535b-415a-b674-995e150fc955",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
