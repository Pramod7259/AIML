{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ecd54dd-967b-4592-b1e8-976d157d3c4f",
   "metadata": {},
   "source": [
    "# Write a NLP Program to demostrate following tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29f67e6-a7ac-4037-9197-e0a7d0293b08",
   "metadata": {},
   "source": [
    "## a. Tokenization removal of stop words, punchuation, POS & NER Tags¶\n",
    "## b. Bag of Words, TF-IDF Vectorisation & Ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b353c8e-4d1e-4a29-9e04-9d8716fd493c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d3e2da2-f966-48b0-8eee-9f560d10d358",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d86fbef-a7b9-4820-87bd-f4f9ee6c72b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "\n",
    "import re\n",
    "#import nltk\n",
    "#from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b29215-cc06-4aae-bd75-c971042ef70a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d29a1f79-6b0f-4532-9731-38d267b4c929",
   "metadata": {},
   "source": [
    "## Download required NLTK resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28fbe0ba-359e-4da7-a0d2-cb573ebb55a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Pramoda A\n",
      "[nltk_data]     S\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Pramoda A\n",
      "[nltk_data]     S\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Pramoda A S\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to C:\\Users\\Pramoda\n",
      "[nltk_data]     A S\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to C:\\Users\\Pramoda A\n",
      "[nltk_data]     S\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK stopwords if not already\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac821782-4340-4b4c-a0ea-f7ef7eadbd8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86308a5d-6a42-447f-bf71-077980d4b2aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f3ec2bf-9670-4f43-a085-369a9d1b55a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in  python bash :- python -m spacy download en_core_web_sm\n",
    "\n",
    "# Load spacy model\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb66fb0a-d9eb-4203-9e36-3668cd783dc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12cf8474-29e6-44e5-adac-0cac7d409f74",
   "metadata": {},
   "source": [
    "## Sample Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da4c1135-5b72-47c5-8410-8614d1edc730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text\n",
    "\n",
    "text = \"\"\"Apple is looking at buying U.K. startup for $1 billion. \n",
    "          Artificial Intelligence is the future of technology!\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ae2356-8f38-45f5-b7c0-71daa15dd458",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef8d6e4d-009c-47c1-a6ea-bc5484ff0d47",
   "metadata": {},
   "source": [
    "## a) Tokenization, Stop Words & Punctuation Removal\n",
    "\n",
    "#   ------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b177347a-97cd-4cf3-b1b3-3d197e0c8356",
   "metadata": {},
   "source": [
    "## Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16e901ab-4674-4c32-a1aa-d2fa0ea9eb19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Tokens: ['Apple', 'is', 'looking', 'at', 'buying', 'U.K.', 'startup', 'for', '$', '1', 'billion', '.', '\\n          ', 'Artificial', 'Intelligence', 'is', 'the', 'future', 'of', 'technology', '!']\n"
     ]
    }
   ],
   "source": [
    "# ---------------- 1. Tokenization ----------------\n",
    "\n",
    "doc = nlp(text)\n",
    "tokens = [token.text for token in doc]\n",
    "print(\"\\n1. Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "521388d2-e335-4b23-a485-3b3799c1ad00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Tokens:\n",
      "['Apple', 'is', 'looking', 'at', 'buying', 'U.K.', 'startup', 'for', '$', '1', 'billion', '.', 'Artificial', 'Intelligence', 'is', 'the', 'future', 'of', 'technology', '!']\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(text)\n",
    "print(\"1. Tokens:\")\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e87880-1999-40cf-95cf-c0ced1fb392e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f047c7a4-4ef4-40ba-8da4-63ae0def9e03",
   "metadata": {},
   "source": [
    "## Remove stop words and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e2f5b98-f397-4fcb-bf27-fb3e3e118fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. After Stopword and Punctuation Removal: ['Apple', 'looking', 'buying', 'startup', 'billion', 'Artificial', 'Intelligence', 'future', 'technology']\n"
     ]
    }
   ],
   "source": [
    "# ---------------- 2. Stopword and Punctuation Removal ----------------\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "filtered_tokens = [token for token in tokens if token.lower() not in stop_words and token.isalpha()]\n",
    "print(\"\\n3. After Stopword and Punctuation Removal:\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303e1456-d3a4-4765-95d1-252756c8b818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a085d9a-cce7-4726-aac4-ee913b6ab67e",
   "metadata": {},
   "source": [
    "## POS Tagging & NER using Spacy\n",
    "# ------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4be061-48d9-424c-8437-80222a77a3f3",
   "metadata": {},
   "source": [
    "## POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25b16af3-de8a-4f97-9bc7-d70d179ef293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. POS Tagging:\n",
      "Apple           -> PROPN\n",
      "is              -> AUX\n",
      "looking         -> VERB\n",
      "at              -> ADP\n",
      "buying          -> VERB\n",
      "U.K.            -> PROPN\n",
      "startup         -> VERB\n",
      "for             -> ADP\n",
      "$               -> SYM\n",
      "1               -> NUM\n",
      "billion         -> NUM\n",
      ".               -> PUNCT\n",
      "\n",
      "               -> SPACE\n",
      "Artificial      -> PROPN\n",
      "Intelligence    -> PROPN\n",
      "is              -> AUX\n",
      "the             -> DET\n",
      "future          -> NOUN\n",
      "of              -> ADP\n",
      "technology      -> NOUN\n",
      "!               -> PUNCT\n"
     ]
    }
   ],
   "source": [
    "# ---------------- 3. POS Tagging ----------------\n",
    "\n",
    "print(\"\\n3. POS Tagging:\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text:15} -> {token.pos_}\")\n",
    "\n",
    "\n",
    "# PROPN → Proper Noun\n",
    "## Example: Apple, New York, AI\n",
    "## (Names of people, organizations, places, etc.)\n",
    "\n",
    "# AUX → Auxiliary Verb (helping verb)\n",
    "## Example: is, was, have, will\n",
    "## (Used with main verbs to form tenses, moods, voices.)\n",
    "\n",
    "# VERB → Verb\n",
    "## Example: looking, buying, expand\n",
    "## (Action or state words.)\n",
    "\n",
    "# ADP → Adposition (prepositions and postpositions)\n",
    "## Example: at, in, on, under\n",
    "## (Show relationship in time/place/direction.)\n",
    "\n",
    "# OUN → Common Noun\n",
    "## Example: startup, business\n",
    "## (General things, not proper names.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d2e65d-9139-42c9-a759-5e5e99719a40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d28ccd9-a173-4f7a-afd3-20dbbb31a85e",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89d5bd33-e9fb-462a-9255-6c5c9e185089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. Named Entities:\n",
      "Apple                -> ORG\n",
      "U.K.                 -> GPE\n",
      "$1 billion           -> MONEY\n",
      "Artificial Intelligence -> PERSON\n"
     ]
    }
   ],
   "source": [
    "# ---------------- 4. Named Entity Recognition (NER) ----------------\n",
    "\n",
    "print(\"\\n4. Named Entities:\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text:20} -> {ent.label_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80033f84-77b3-4a7e-9ed0-68bd654ac6df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37db39f-fb8a-49c5-bbe0-9aadbffb6559",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee6cc261-e70e-43ad-857c-576e4bffc917",
   "metadata": {},
   "source": [
    "## -------------------------------\n",
    "# Part B: Bag of Words & TF-IDF Vectorization\n",
    "# -------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3842e443-3c92-41de-a860-be0a94fbdfe8",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7da63f0-a50c-4953-bdb6-fb268880451d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Bag of Words ---\n",
      "['apple' 'artificial' 'at' 'billion' 'buying' 'for' 'future'\n",
      " 'intelligence' 'is' 'looking' 'of' 'startup' 'technology' 'the']\n",
      "[[1 1 1 1 1 1 1 1 2 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "bow = vectorizer.fit_transform([text])\n",
    "print(\"\\n--- Bag of Words ---\")\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(bow.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c235543-0553-4956-bf0e-5ee0f0683d74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d5c7b1c-4b94-47b1-b687-81a4f5383516",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "362078ac-2413-4b01-9ac5-4746c4eff001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TF-IDF ---\n",
      "['apple' 'artificial' 'at' 'billion' 'buying' 'for' 'future'\n",
      " 'intelligence' 'is' 'looking' 'of' 'startup' 'technology' 'the']\n",
      "[[0.24253563 0.24253563 0.24253563 0.24253563 0.24253563 0.24253563\n",
      "  0.24253563 0.24253563 0.48507125 0.24253563 0.24253563 0.24253563\n",
      "  0.24253563 0.24253563]]\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf = tfidf_vectorizer.fit_transform([text])\n",
    "print(\"\\n--- TF-IDF ---\")\n",
    "print(tfidf_vectorizer.get_feature_names_out())\n",
    "print(tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79813f7-dc25-4ec3-b5af-92fe370aacb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc4d96fe-3246-43f5-8bd3-7f71b00f71fe",
   "metadata": {},
   "source": [
    "## N-grams (Bigrams and Trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159b171c-4d9c-42fa-93d1-cb665066d95e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1dcfda04-bd61-4e64-9509-f4792932b7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. N-grams:\n",
      "\n",
      "Unigrams:\n",
      " [('Apple',), ('looking',), ('buying',), ('startup',), ('billion',), ('Artificial',), ('Intelligence',), ('future',), ('technology',)]\n",
      "\n",
      "\n",
      "Bigrams:\n",
      " [('Apple', 'looking'), ('looking', 'buying'), ('buying', 'startup'), ('startup', 'billion'), ('billion', 'Artificial'), ('Artificial', 'Intelligence'), ('Intelligence', 'future'), ('future', 'technology')]\n",
      "\n",
      "\n",
      "Trigrams:\n",
      " [('Apple', 'looking', 'buying'), ('looking', 'buying', 'startup'), ('buying', 'startup', 'billion'), ('startup', 'billion', 'Artificial'), ('billion', 'Artificial', 'Intelligence'), ('Artificial', 'Intelligence', 'future'), ('Intelligence', 'future', 'technology')]\n"
     ]
    }
   ],
   "source": [
    "# ---------------- 5. N-grams Generation ----------------\n",
    "\n",
    "# Example: bigrams (n=2) and trigrams (n=3)\n",
    "\n",
    "print(\"\\n5. N-grams:\\n\")\n",
    "unigrams = list(ngrams(filtered_tokens, 1))\n",
    "bigrams = list(ngrams(filtered_tokens, 2))\n",
    "trigrams = list(ngrams(filtered_tokens, 3))\n",
    "\n",
    "print(\"Unigrams:\\n\", unigrams)\n",
    "print(\"\\n\\nBigrams:\\n\", bigrams)\n",
    "print(\"\\n\\nTrigrams:\\n\", trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904fabca-c4c1-4fd4-83ee-09f2dab440d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d24d40-5de7-42b5-8203-19100e8aaf63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc900693-28dc-44ba-938f-d86076f6ae80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
