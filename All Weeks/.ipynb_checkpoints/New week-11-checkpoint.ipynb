{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d86fbef-a7b9-4820-87bd-f4f9ee6c72b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28fbe0ba-359e-4da7-a0d2-cb573ebb55a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Pramoda A\n",
      "[nltk_data]     S\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK stopwords if not already\n",
    "\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f3ec2bf-9670-4f43-a085-369a9d1b55a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in  python bash :- python -m spacy download en_core_web_sm\n",
    "\n",
    "# Load spacy model\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da4c1135-5b72-47c5-8410-8614d1edc730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text\n",
    "\n",
    "text = \"Apple is looking at buying a startup in New York to expand its AI business.\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "b0809a01-bd08-4ba4-a04a-bf9cbdb0b60c",
   "metadata": {},
   "source": [
    "# ---------------- 1. Tokenization ----------------\n",
    "\n",
    "doc = nlp(text)\n",
    "tokens = [token.text for token in doc]\n",
    "print(\"\\n1. Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "521388d2-e335-4b23-a485-3b3799c1ad00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Tokens:\n",
      "['Apple', 'is', 'looking', 'at', 'buying', 'a', 'startup', 'in', 'New', 'York', 'to', 'expand', 'its', 'AI', 'business', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(text)\n",
    "print(\"1. Tokens:\")\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e2f5b98-f397-4fcb-bf27-fb3e3e118fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. After Stopword Removal: ['Apple', 'looking', 'buying', 'startup', 'New', 'York', 'expand', 'AI', 'business']\n"
     ]
    }
   ],
   "source": [
    "# ---------------- 2. Stopword Removal ----------------\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "filtered_tokens = [token for token in tokens if token.lower() not in stop_words and token.isalpha()]\n",
    "print(\"\\n2. After Stopword Removal:\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303e1456-d3a4-4765-95d1-252756c8b818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "13a01d5d-eadf-4a0f-9f20-c24a1a54db0c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "25b16af3-de8a-4f97-9bc7-d70d179ef293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. POS Tagging:\n",
      "Apple           -> PROPN\n",
      "is              -> AUX\n",
      "looking         -> VERB\n",
      "at              -> ADP\n",
      "buying          -> VERB\n",
      "a               -> DET\n",
      "startup         -> NOUN\n",
      "in              -> ADP\n",
      "New             -> PROPN\n",
      "York            -> PROPN\n",
      "to              -> PART\n",
      "expand          -> VERB\n",
      "its             -> PRON\n",
      "AI              -> PROPN\n",
      "business        -> NOUN\n",
      ".               -> PUNCT\n"
     ]
    }
   ],
   "source": [
    "# PROPN → Proper Noun\n",
    "## Example: Apple, New York, AI\n",
    "## (Names of people, organizations, places, etc.)\n",
    "\n",
    "# AUX → Auxiliary Verb (helping verb)\n",
    "## Example: is, was, have, will\n",
    "## (Used with main verbs to form tenses, moods, voices.)\n",
    "\n",
    "# VERB → Verb\n",
    "## Example: looking, buying, expand\n",
    "## (Action or state words.)\n",
    "\n",
    "# ADP → Adposition (prepositions and postpositions)\n",
    "## Example: at, in, on, under\n",
    "## (Show relationship in time/place/direction.)\n",
    "\n",
    "# OUN → Common Noun\n",
    "## Example: startup, business\n",
    "## (General things, not proper names.)\n",
    "\n",
    "\n",
    "# ---------------- 3. POS Tagging ----------------\n",
    "\n",
    "print(\"\\n3. POS Tagging:\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text:15} -> {token.pos_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "89d5bd33-e9fb-462a-9255-6c5c9e185089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. Named Entities:\n",
      "Apple                -> ORG\n",
      "New York             -> GPE\n",
      "AI                   -> GPE\n"
     ]
    }
   ],
   "source": [
    "# ---------------- 4. Named Entity Recognition (NER) ----------------\n",
    "\n",
    "print(\"\\n4. Named Entities:\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text:20} -> {ent.label_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1dcfda04-bd61-4e64-9509-f4792932b7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. N-grams:\n",
      "\n",
      "Bigrams:\n",
      " [('Apple', 'looking'), ('looking', 'buying'), ('buying', 'startup'), ('startup', 'New'), ('New', 'York'), ('York', 'expand'), ('expand', 'AI'), ('AI', 'business')]\n",
      "\n",
      "\n",
      "Trigrams:\n",
      " [('Apple', 'looking', 'buying'), ('looking', 'buying', 'startup'), ('buying', 'startup', 'New'), ('startup', 'New', 'York'), ('New', 'York', 'expand'), ('York', 'expand', 'AI'), ('expand', 'AI', 'business')]\n"
     ]
    }
   ],
   "source": [
    "# ---------------- 5. N-grams Generation ----------------\n",
    "\n",
    "# Example: bigrams (n=2) and trigrams (n=3)\n",
    "\n",
    "print(\"\\n5. N-grams:\\n\")\n",
    "bigrams = list(ngrams(filtered_tokens, 2))\n",
    "trigrams = list(ngrams(filtered_tokens, 3))\n",
    "\n",
    "print(\"Bigrams:\\n\", bigrams)\n",
    "print(\"\\n\\nTrigrams:\\n\", trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904fabca-c4c1-4fd4-83ee-09f2dab440d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d24d40-5de7-42b5-8203-19100e8aaf63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc900693-28dc-44ba-938f-d86076f6ae80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
